{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04736b8a",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "## Introduction to PySpark\n",
    "\n",
    "`Apache Spark` is an open-source, distributed computing system designed for fast processing of large scale data. `PySpark` is the Python interface for Apache Spark that allows for handling of large datasets efficiently with parallel computation in Python workflows, ideal for batch processing, real-time streaming, machine learning, data analytics, and SQL queries.\n",
    "\n",
    "## When would we use PySpark?\n",
    "\n",
    "`PySpark` is ideal for handling large datasets that do not fit into memory, as it can distribute data and computations across a cluster of machines. It excels in: Big Data Analytics through Distributed Data Processing, using Spark's in-memory computation for faster processing. Machine Learning on Large Datasets leverages Spark's MLlib library for scalable machine learning algorithms. ELT and ETL pipelines transforms large volumes of raw data from sources into structured formats. PySpark is flexible, working with diverse data sources like CSVs, JSON, Parquet files, and databases.\n",
    "\n",
    "## Spark Clusters\n",
    "\n",
    "A key component of working with `PySpark` is clusters. A Spark cluster is a group of computers (nodes) that collaboratively process large datasets using Apache Spark, with a master node coordinating multiple worker nodes. This architecture enables distributed processing. The master node manages resources and tasks, while worker nodes execute assigned compute tasks.\n",
    "\n",
    "## SparkSession\n",
    "\n",
    "A `SparkSession` is the entry point to programming with `PySpark`. It allows you to create DataFrames, execute SQL queries, and manage Spark applications. You can create a `SparkSession` using the following code:\n",
    "\n",
    "```python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").getOrCreate()\n",
    "\n",
    "```\n",
    "\n",
    "`.builder` sets up a session, `getOrCreate()` creates it if it doesn't exist, and `appName` helps manage multiple sessions.\n",
    "\n",
    "It is best practice to use `SparkSession.builder.getOrCreate()` which returns an existing session or creates a new one if necessary. This avoids creating multiple sessions in the same application, which can lead to resource conflicts and inefficiencies.\n",
    "\n",
    "## PySpark DataFrames\n",
    "\n",
    "`PySpark DataFrames` are distributed, table-like structures optimized for large-scale data processing. Their syntax is similar to Pandas DataFrames, with the main difference being how data is managed at a low level. \n",
    "\n",
    "To create a PySpark DataFrame, we use the `spark.read.csv()` function in the Spark Session with a CSV file.\n",
    "\n",
    "```python\n",
    "# Import and initialize a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "census_df = spark.read.csv(\"census.csv\", [\"gender\", \"age\", \"zipcode\", \"salary_range_usd\", \"marriage_status\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "census_df.show()\n",
    "```\n",
    "\n",
    "Pandas operates on a single compute instance, while PySpark distributes data and computations across a cluster of machines, enabling efficient processing of large datasets that exceed the memory capacity of a single machine.\n",
    "\n",
    "DataFrames are essential in `PySpark` for efficiently managing large-scale data across clusters. While they resemble Pandas DataFrames in structure and syntax, they are optimized for distributed computing, allowing for scalable data processing and analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
